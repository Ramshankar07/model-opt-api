{
  "title": "Post-Training Optimization Taxonomy (Truly Calibration-Free)",
  "description": "Zero-Data Methods Only - No Real or Synthetic Data Required",
  "source": "s.txt (lines 10-421)",
  "tree_text": "Post-Training Optimization Taxonomy (Truly Calibration-Free)\n│\n├── 1. CNN-Based Models\n│   ├── 1.1 Classification Models\n│   │   ├── ResNet Family\n│   │   │   ├── Quantization\n│   │   │   │   ├── Weight-Only Quantization (W8/W4)\n│   │   │   │   ├── AdpQ (Adaptive LASSO, W3/W4)\n│   │   │   │   ├── VLCQ (Variable-Length Coding)\n│   │   │   │   ├── Hardware-Friendly PTQ (2/4-bit Channel-wise)\n│   │   │   │   └── Per-Channel Weight Quantization\n│   │   │   │\n│   │   │   ├── Fusion\n│   │   │   │   ├── Conv-BN Fusion\n│   │   │   │   ├── Conv-ReLU Fusion\n│   │   │   │   ├── Graph Fusion (Conv+BN+ReLU)\n│   │   │   │   └── Residual Connection Fusion\n│   │   │   │\n│   │   │   ├── Pruning\n│   │   │   │   ├── Channel Pruning (L1-norm Based)\n│   │   │   │   ├── Channel Pruning (BN Scale Factor)\n│   │   │   │   ├── Filter Pruning (Weight Magnitude)\n│   │   │   │   └── Structured Pruning (Layer-wise)\n│   │   │   │\n│   │   │   └── Structural\n│   │   │       ├── Skip Connection Optimization (Tailor)\n│   │   │       └── Bottleneck Restructuring\n│   │   │\n│   │   ├── VGG Family\n│   │   │   ├── Quantization\n│   │   │   │   ├── Weight-Only Quantization (W8/W4)\n│   │   │   │   ├── Weight Clustering (K-means)\n│   │   │   │   ├── Codebook Quantization\n│   │   │   │   └── Per-Layer Weight Quantization\n│   │   │   │\n│   │   │   ├── Fusion\n│   │   │   │   ├── Conv-BN Fusion\n│   │   │   │   ├── Conv-ReLU Fusion\n│   │   │   │   └── Sequential Layer Fusion\n│   │   │   │\n│   │   │   └── Pruning\n│   │   │       ├── Filter Pruning (Weight Magnitude)\n│   │   │       └── Structured Layer Pruning\n│   │   │\n│   │   └── MobileNet Family\n│   │       ├── Quantization\n│   │       │   ├── Weight-Only Quantization (W8/W4)\n│   │       │   ├── Depthwise Conv Quantization\n│   │       │   ├── Pointwise Conv Quantization\n│   │       │   └── Inverted Residual Quantization\n│   │       │\n│   │       ├── Fusion\n│   │       │   ├── Conv-BN Fusion\n│   │       │   ├── Depthwise-Pointwise Fusion\n│   │       │   └── Inverted Residual Fusion\n│   │       │\n│   │       ├── Pruning\n│   │       │   ├── Channel Pruning (Depthwise)\n│   │       │   └── Width Multiplier Adjustment\n│   │       │\n│   │       └── Structural\n│   │           └── Bottleneck Optimization\n│   │\n│   ├── 1.2 Detection Models\n│   │   ├── YOLO Series\n│   │   │   ├── Quantization\n│   │   │   │   ├── Weight-Only Quantization (Backbone)\n│   │   │   │   ├── Weight-Only Quantization (Neck)\n│   │   │   │   ├── Weight-Only Quantization (Head)\n│   │   │   │   └── Anchor-Free Head Quantization\n│   │   │   │\n│   │   │   ├── Fusion\n│   │   │   │   ├── Conv-BN Fusion (Backbone)\n│   │   │   │   ├── Conv-BN Fusion (Neck)\n│   │   │   │   ├── Feature Fusion Layer Optimization\n│   │   │   │   └── Path Aggregation Fusion\n│   │   │   │\n│   │   │   ├── Pruning\n│   │   │   │   ├── Channel Pruning (L1-norm)\n│   │   │   │   ├── Channel Pruning (Group-wise)\n│   │   │   │   └── Backbone Layer Pruning\n│   │   │   │\n│   │   │   └── Structural\n│   │   │       ├── NMS Acceleration (QSI-NMS)\n│   │   │       ├── NMS Acceleration (eQSI-NMS)\n│   │   │       └── Detection Head Optimization\n│   │   │\n│   │   ├── SSD Family\n│   │   │   ├── Quantization\n│   │   │   │   ├── Weight-Only Quantization (Backbone)\n│   │   │   │   ├── Multi-scale Feature Quantization\n│   │   │   │   └── Default Box Prediction Quantization\n│   │   │   │\n│   │   │   ├── Fusion\n│   │   │   │   ├── Conv-BN Fusion\n│   │   │   │   └── Multi-scale Feature Fusion\n│   │   │   │\n│   │   │   └── Structural\n│   │   │       └── NMS Acceleration\n│   │   │\n│   │   └── RetinaNet\n│   │       ├── Quantization\n│   │       │   ├── Weight-Only Quantization (Backbone)\n│   │       │   ├── FPN Weight Quantization\n│   │       │   └── Classification Subnet Quantization\n│   │       │\n│   │       ├── Fusion\n│   │       │   ├── Conv-BN Fusion\n│   │       │   └── FPN Layer Fusion\n│   │       │\n│   │       └── Structural\n│   │           └── NMS Acceleration\n│   │\n│   └── 1.3 Segmentation Models\n│       ├── U-Net Family\n│       │   ├── Quantization\n│       │   │   ├── Weight-Only Quantization (Encoder)\n│       │   │   ├── Weight-Only Quantization (Decoder)\n│       │   │   └── Skip Connection Quantization\n│       │   │\n│       │   ├── Fusion\n│       │   │   ├── Conv-BN Fusion (Encoder)\n│       │   │   ├── Conv-BN Fusion (Decoder)\n│       │   │   ├── Skip Connection Fusion\n│       │   │   └── Upsampling Layer Fusion\n│       │   │\n│       │   └── Structural\n│       │       └── Skip Connection Optimization (Tailor)\n│       │\n│       ├── DeepLab Series\n│       │   ├── Quantization\n│       │   │   ├── Weight-Only Quantization (Backbone)\n│       │   │   ├── ASPP Module Quantization\n│       │   │   └── Atrous Convolution Quantization\n│       │   │\n│       │   ├── Fusion\n│       │   │   ├── Conv-BN Fusion (Backbone)\n│       │   │   ├── ASPP Module Fusion\n│       │   │   └── Decoder Fusion\n│       │   │\n│       │   └── Pruning\n│       │       └── ASPP Branch Pruning\n│       │\n│       └── FCN Family\n│           ├── Quantization\n│           │   ├── Weight-Only Quantization (Backbone)\n│           │   └── Transposed Conv Quantization\n│           │\n│           ├── Fusion\n│           │   ├── Conv-BN Fusion\n│           │   ├── Skip Layer Fusion\n│           │   └── Upsampling Fusion\n│           │\n│           └── Pruning\n│               └── Skip Layer Pruning\n│\n├── 2. Transformer-Based Models\n│   ├── 2.1 Vision Transformers (ViT)\n│   │   ├── Vanilla ViT\n│   │   │   ├── Quantization\n│   │   │   │   ├── BoA (Attention-aware Hessian)\n│   │   │   │   ├── aespa (Attention-wise Reconstruction)\n│   │   │   │   ├── Weight-Only Quantization (Linear Layers)\n│   │   │   │   ├── Weight-Only Quantization (QKV Projection)\n│   │   │   │   ├── Weight-Only Quantization (MLP)\n│   │   │   │   ├── Attention Weight Quantization\n│   │   │   │   ├── Patch Embedding Quantization\n│   │   │   │   └── Per-Layer Weight Quantization\n│   │   │   │\n│   │   │   ├── Fusion\n│   │   │   │   ├── LayerNorm Fusion\n│   │   │   │   ├── QKV Projection Fusion\n│   │   │   │   └── MLP Fusion\n│   │   │   │\n│   │   │   ├── Pruning\n│   │   │   │   ├── Attention Head Pruning (Weight-Based)\n│   │   │   │   ├── MLP Dimension Pruning\n│   │   │   │   └── Layer Pruning (Depth Reduction)\n│   │   │   │\n│   │   │   └── Structural\n│   │   │       ├── Patch Merging\n│   │   │       └── Token Dimension Reduction\n│   │   │\n│   │   ├── Swin Transformer\n│   │   │   ├── Quantization\n│   │   │   │   ├── BoA (Attention-aware Hessian)\n│   │   │   │   ├── aespa (Attention-wise Reconstruction)\n│   │   │   │   ├── Weight-Only Quantization\n│   │   │   │   ├── Window Attention Weight Quantization\n│   │   │   │   ├── Shifted Window Mechanism Quantization\n│   │   │   │   └── Hierarchical Feature Quantization\n│   │   │   │\n│   │   │   ├── Fusion\n│   │   │   │   ├── LayerNorm Fusion\n│   │   │   │   ├── Window Partition Fusion\n│   │   │   │   └── Patch Merging Fusion\n│   │   │   │\n│   │   │   └── Pruning\n│   │   │       ├── Window Attention Head Pruning\n│   │   │       └── Stage Pruning\n│   │   │\n│   │   ├── DeiT Family\n│   │   │   ├── Quantization\n│   │   │   │   ├── BoA (Attention-aware Hessian)\n│   │   │   │   ├── Weight-Only Quantization\n│   │   │   │   └── Distillation Token Quantization\n│   │   │   │\n│   │   │   ├── Fusion\n│   │   │   │   └── LayerNorm Fusion\n│   │   │   │\n│   │   │   └── Structural\n│   │   │       └── Distillation Token Removal\n│   │   │\n│   │   └── BEiT & MAE\n│   │       ├── Quantization\n│   │       │   ├── Weight-Only Quantization (Encoder)\n│   │       │   └── Masked Token Quantization\n│   │       │\n│   │       └── Structural\n│   │           └── Reconstruction Head Removal\n│   │\n│   ├── 2.2 Detection Transformers\n│   │   └── DETR Family\n│   │       ├── Quantization\n│   │       │   ├── Weight-Only Quantization (Encoder)\n│   │       │   ├── Weight-Only Quantization (Decoder)\n│   │       │   ├── Object Query Quantization\n│   │       │   ├── Cross-Attention Quantization\n│   │       │   └── Self-Attention Quantization\n│   │       │\n│   │       └── Fusion\n│   │           └── Encoder-Decoder Fusion\n│   │\n│   └── 2.3 Segmentation Transformers\n│       └── Mask2Former\n│           ├── Quantization\n│           │   ├── Weight-Only Quantization (Swin Backbone)\n│           │   ├── Pixel Decoder Quantization\n│           │   └── Transformer Decoder Quantization\n│           │\n│           └── Fusion\n│               └── Multi-scale Feature Fusion\n│\n├── 3. Hybrid Architectures\n│   ├── 3.1 CNN-Transformer Hybrids\n│   │   ├── CoAtNet\n│   │   │   ├── Quantization\n│   │   │   │   ├── Weight-Only Quantization (CNN Blocks)\n│   │   │   │   ├── Weight-Only Quantization (Transformer Blocks)\n│   │   │   │   ├── MBConv Quantization\n│   │   │   │   └── Relative Attention Quantization\n│   │   │   │\n│   │   │   ├── Fusion\n│   │   │   │   ├── Conv-BN Fusion (CNN Blocks)\n│   │   │   │   ├── MBConv Fusion\n│   │   │   │   └── Stage Transition Fusion\n│   │   │   │\n│   │   │   └── Pruning\n│   │   │       ├── Channel Pruning (CNN Blocks)\n│   │   │       └── Attention Head Pruning (Transformer Blocks)\n│   │   │\n│   │   └── MobileViT\n│   │       ├── Quantization\n│   │       │   ├── Weight-Only Quantization (MobileNet Blocks)\n│   │       │   ├── Weight-Only Quantization (ViT Blocks)\n│   │       │   ├── QADS (Per-channel Scaling)\n│   │       │   ├── Depthwise Separable Quantization\n│   │       │   └── Local-Global Feature Quantization\n│   │       │\n│   │       ├── Fusion\n│   │       │   ├── Conv-BN Fusion (MobileNet Blocks)\n│   │       │   ├── Depthwise-Pointwise Fusion\n│   │       │   └── Transformer-Conv Transition Fusion\n│   │       │\n│   │       └── Pruning\n│   │           ├── Channel Pruning (MobileNet Blocks)\n│   │           └── Attention Head Pruning (ViT Blocks)\n│   │\n│   └── 3.2 Convolution-Enhanced Transformers\n│       └── CvT (Convolutional Vision Transformer)\n│           ├── Quantization\n│           │   ├── Weight-Only Quantization\n│           │   ├── Convolutional Token Embedding Quantization\n│           │   └── Convolutional Projection Quantization\n│           │\n│           ├── Fusion\n│           │   ├── Convolutional Projection Fusion\n│           │   └── Hierarchical Stage Fusion\n│           │\n│           └── Pruning\n│               └── Stage Pruning\n│\n├── 4. Multimodal Models\n│   ├── 4.1 Vision-Language Models\n│   │   ├── CLIP Family\n│   │   │   ├── Quantization\n│   │   │   │   ├── Weight-Only Quantization (Vision Encoder)\n│   │   │   │   ├── Weight-Only Quantization (Text Encoder)\n│   │   │   │   ├── Projection Layer Quantization\n│   │   │   │   ├── Cross-Modal Attention Quantization\n│   │   │   │   └── Contrastive Learning Head Quantization\n│   │   │   │\n│   │   │   ├── Fusion\n│   │   │   │   ├── Conv-BN Fusion (Vision Encoder)\n│   │   │   │   └── Projection Layer Fusion\n│   │   │   │\n│   │   │   └── Pruning\n│   │   │       ├── Vision Encoder Pruning\n│   │   │       └── Text Encoder Pruning\n│   │   │\n│   │   └── BLIP Family\n│   │       ├── Quantization\n│   │       │   ├── Weight-Only Quantization (Vision Encoder)\n│   │       │   ├── Weight-Only Quantization (Q-Former)\n│   │       │   ├── Weight-Only Quantization (Text Decoder)\n│   │       │   └── Cross-Attention Quantization\n│   │       │\n│   │       └── Fusion\n│   │           └── Multi-Modal Feature Fusion\n│   │\n│   ├── 4.2 Text-to-Image Models\n│   │   └── Stable Diffusion\n│   │       ├── Quantization\n│   │       │   ├── ViDiT-Q Dynamic (Timestep-aware)\n│   │       │   ├── Weight-Only Quantization (U-Net Denoiser)\n│   │       │   ├── Weight-Only Quantization (VAE Encoder)\n│   │       │   ├── Weight-Only Quantization (VAE Decoder)\n│   │       │   ├── Weight-Only Quantization (Text Encoder)\n│   │       │   ├── Cross-Attention Quantization\n│   │       │   ├── Self-Attention Quantization\n│   │       │   ├── Timestep Embedding Quantization\n│   │       │   └── Channel-wise Dynamic Quantization\n│   │       │\n│   │       ├── Fusion\n│   │       │   ├── Conv-BN Fusion (U-Net Backbone)\n│   │       │   ├── ResBlock Fusion\n│   │       │   ├── Attention Block Fusion\n│   │       │   └── VAE Layer Fusion\n│   │       │\n│   │       └── Structural\n│   │           ├── Timestep-aware Parameter Adjustment\n│   │           └── Static-Dynamic Channel Balancing\n│   │\n│   └── 4.3 Video-Language Models\n│       └── VideoCLIP\n│           ├── Quantization\n│           │   ├── Weight-Only Quantization (Vision Encoder)\n│           │   ├── Weight-Only Quantization (Text Encoder)\n│           │   ├── Temporal Attention Quantization\n│           │   └── 3D Convolution Quantization\n│           │\n│           ├── Fusion\n│           │   ├── Conv-BN Fusion\n│           │   ├── 3D Conv Fusion\n│           │   └── Temporal Feature Fusion\n│           │\n│           └── Pruning\n│               └── Frame Sampling Optimization\n│\n└── 5. Specialized Architectures\n    ├── 5.1 Efficient Architectures\n    │   ├── RegNet\n    │   │   ├── Quantization\n    │   │   │   ├── Weight-Only Quantization\n    │   │   │   └── AnyNet Block Quantization\n    │   │   │\n    │   │   ├── Fusion\n    │   │   │   ├── Conv-BN Fusion\n    │   │   │   └── Stem Fusion\n    │   │   │\n    │   │   └── Pruning\n    │   │       ├── Block Pruning (Weight-Based)\n    │   │       └── Stage Pruning\n    │   │\n    │   └── ConvNeXt\n    │       ├── Quantization\n    │       │   ├── Weight-Only Quantization\n    │       │   ├── Depthwise Conv Quantization\n    │       │   └── Large Kernel Quantization\n    │       │\n    │       ├── Fusion\n    │       │   ├── Layer Scale Fusion\n    │       │   └── Inverted Bottleneck Fusion\n    │       │\n    │       └── Pruning\n    │           └── Block Pruning\n    │\n    └── 5.2 Neural Architecture Search Models\n        ├── MNasNet\n        │   ├── Quantization\n        │   │   ├── Weight-Only Quantization\n        │   │   └── Separable Conv Quantization\n        │   │\n        │   ├── Fusion\n        │   │   ├── Conv-BN Fusion\n        │   │   └── Depthwise-Pointwise Fusion\n        │   │\n        │   └── Pruning\n        │       └── Channel Pruning\n        │\n        └── ProxylessNAS\n            ├── Quantization\n            │   ├── Weight-Only Quantization\n            │   └── Hardware-Aware Quantization\n            │\n            ├── Fusion\n            │   └── Conv-BN Fusion\n            │\n            └── Pruning\n                └── Path Pruning\n",
  "created_at": "2025-01-06T15:38:00"
}

